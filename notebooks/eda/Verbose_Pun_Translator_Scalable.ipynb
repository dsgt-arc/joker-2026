{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé≠ Pun Translator (Scalable: Lexique3 + ConceptNet + WordNet)\n",
    "\n",
    "**Verbose, reproducible pun translation without a hand-written pun dictionary.**\n",
    "\n",
    "This version removes the hard-coded `known_puns`, `ipa_map`, and `relatedness` tables and replaces them with:\n",
    "\n",
    "- **WordNet (English)** to propose candidate pun words + two distinct senses automatically\n",
    "- **Lexique 3 (French)** to find homophones by shared phonological form (offline, fast)\n",
    "- **ConceptNet (free API)** to score semantic relatedness between the homophone and the target meaning\n",
    "\n",
    "‚ö†Ô∏è One-time setup: download **Lexique 3** from lexique.org and point the notebook to the TSV/CSV file path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q deep-translator requests pandas nltk\n",
    "print('‚úÖ Installed dependencies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Summary: imports + core classes + translator engine\n",
    "\n",
    "Load the libraries, ensures WordNet is available, and defines the main translation components: \n",
    "an English‚ÜíFrench translator with caching, a Lexique-based French homophone index, a \n",
    "ConceptNet-based semantic similarity scorer, and a verbose ‚Äúpolygon‚Äù solver that tries \n",
    "progressively smarter ways to preserve a pun in French.\n",
    "'''\n",
    "\n",
    "# Imports + WordNet bootstrap / dependencies\n",
    "\n",
    "from deep_translator import GoogleTranslator\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import difflib\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# --- NLTK data (WordNet) ---\n",
    "# Free and only needs to be downloaded once per runtime.\n",
    "try:\n",
    "    _ = wn.synsets(\"bank\")\n",
    "except LookupError:\n",
    "    nltk.download(\"wordnet\")\n",
    "    nltk.download(\"omw-1.4\")\n",
    "    _ = wn.synsets(\"bank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data models (dataclasses) - structured return objects\n",
    "\n",
    "@dataclass\n",
    "class TranslationCandidate:\n",
    "    pun_word: str\n",
    "    polygon_level: int\n",
    "    path: List[str]\n",
    "    explanation: str\n",
    "    confidence: float\n",
    "\n",
    "@dataclass\n",
    "class FallbackTranslation:\n",
    "    strategy: str\n",
    "    translation: str\n",
    "    explanation: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation wrapper - cached EN‚ÜíFR translation.\n",
    "\n",
    "class RealBilingualDict:\n",
    "    \"\"\"Free translation via deep-translator (Google Translate under the hood).\"\"\"\n",
    "    def __init__(self, source_lang='en', target_lang='fr'):\n",
    "        self.translator = GoogleTranslator(source=source_lang, target=target_lang)\n",
    "        self.cache: Dict[str, List[str]] = {}\n",
    "\n",
    "    def translate(self, word: str) -> List[str]:\n",
    "        word_lower = word.lower().strip()\n",
    "        if not word_lower:\n",
    "            return [word]\n",
    "        if word_lower in self.cache:\n",
    "            return self.cache[word_lower]\n",
    "        try:\n",
    "            translation = self.translator.translate(word_lower)\n",
    "            self.cache[word_lower] = [translation]\n",
    "            time.sleep(0.05)  # be polite to the service\n",
    "            return [translation]\n",
    "        except Exception:\n",
    "            return [word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexique homophone index - builds the offline phonetic lookup.\n",
    "\n",
    "class LexiquePhoneticIndex:\n",
    "    \"\"\"\n",
    "    Offline homophone lookup for French using Lexique 3.\n",
    "\n",
    "    Provide the path to a Lexique 3 file (TSV/CSV). We build:\n",
    "      phon_form -> [orthographic forms]\n",
    "\n",
    "    Lexique columns vary slightly by file; we auto-detect likely columns.\n",
    "    \"\"\"\n",
    "    def __init__(self, lexique_path: str, encoding: str = \"utf-8\"):\n",
    "        if not os.path.exists(lexique_path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Lexique file not found: {lexique_path}\\n\"\n",
    "                \"Download Lexique 3 from lexique.org and update the path.\"\n",
    "            )\n",
    "\n",
    "        # Try TSV first, then CSV\n",
    "        try:\n",
    "            df = pd.read_csv(lexique_path, sep='\\t', encoding=encoding)\n",
    "        except Exception:\n",
    "            df = pd.read_csv(lexique_path, encoding=encoding)\n",
    "\n",
    "        cols = {c.lower(): c for c in df.columns}\n",
    "        ortho_col = cols.get(\"ortho\") or cols.get(\"orth\") or cols.get(\"word\") or cols.get(\"lemme\") or list(df.columns)[0]\n",
    "        phon_col = cols.get(\"phon\") or cols.get(\"phonology\") or cols.get(\"phon_ortho\") or cols.get(\"ipa\") or cols.get(\"phono\")\n",
    "        if phon_col is None:\n",
    "            raise ValueError(\n",
    "                \"Could not find a phonetic column in the Lexique file.\\n\"\n",
    "                \"Expected a column like 'phon' (common in Lexique 3).\\n\"\n",
    "                f\"Columns present: {list(df.columns)[:50]}\"\n",
    "            )\n",
    "\n",
    "        self.ortho_col = ortho_col\n",
    "        self.phon_col = phon_col\n",
    "\n",
    "        self.phon_to_words: Dict[str, List[str]] = {}\n",
    "        self.word_to_phon: Dict[str, str] = {}\n",
    "\n",
    "        for _, row in df[[ortho_col, phon_col]].dropna().iterrows():\n",
    "            w = str(row[ortho_col]).strip().lower()\n",
    "            p = str(row[phon_col]).strip()\n",
    "            if not w or not p:\n",
    "                continue\n",
    "            self.word_to_phon[w] = p\n",
    "            self.phon_to_words.setdefault(p, []).append(w)\n",
    "\n",
    "        for p, words in self.phon_to_words.items():\n",
    "            seen = set()\n",
    "            deduped = []\n",
    "            for w in words:\n",
    "                if w not in seen:\n",
    "                    deduped.append(w)\n",
    "                    seen.add(w)\n",
    "            self.phon_to_words[p] = deduped\n",
    "\n",
    "    def find_homophones(self, french_word: str, limit: int = 30) -> List[str]:\n",
    "        w = french_word.lower().strip()\n",
    "        p = self.word_to_phon.get(w)\n",
    "        if not p:\n",
    "            return []\n",
    "        cands = [x for x in self.phon_to_words.get(p, []) if x != w]\n",
    "        return cands[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConceptNet semantics - does semantic relatedness via a free API + disk cache.\n",
    "\n",
    "class ConceptNetSemantic:\n",
    "    \"\"\"\n",
    "    Free semantic relatedness scoring using ConceptNet's public API.\n",
    "\n",
    "    We approximate similarity by:\n",
    "    - getting a weighted related-term list for word1,\n",
    "    - seeing whether word2 appears in that list (and vice versa),\n",
    "    - using a symmetric score.\n",
    "    \"\"\"\n",
    "    def __init__(self, lang: str = \"fr\", cache_path: str = \"conceptnet_cache.json\"):\n",
    "        self.lang = lang\n",
    "        self.cache_path = cache_path\n",
    "        self._cache: Dict[str, Dict[str, float]] = {}\n",
    "        self._load_cache()\n",
    "\n",
    "    def _load_cache(self):\n",
    "        if os.path.exists(self.cache_path):\n",
    "            try:\n",
    "                with open(self.cache_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    self._cache = json.load(f)\n",
    "            except Exception:\n",
    "                self._cache = {}\n",
    "\n",
    "    def _save_cache(self):\n",
    "        try:\n",
    "            with open(self.cache_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(self._cache, f, ensure_ascii=False, indent=2)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def _related_map(self, word: str, limit: int = 50) -> Dict[str, float]:\n",
    "        w = word.lower().strip()\n",
    "        if not w:\n",
    "            return {}\n",
    "        if w in self._cache:\n",
    "            return self._cache[w]\n",
    "\n",
    "        concept = f\"/c/{self.lang}/{w}\"\n",
    "        url = f\"https://api.conceptnet.io/related{concept}?filter=/c/{self.lang}&limit={limit}\"\n",
    "\n",
    "        try:\n",
    "            r = requests.get(url, timeout=10)\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "            rels = {}\n",
    "            for item in data.get(\"related\", []):\n",
    "                cid = item.get(\"@id\", \"\")\n",
    "                m = re.match(rf\"^/c/{self.lang}/(.+)$\", cid)\n",
    "                if not m:\n",
    "                    continue\n",
    "                term = m.group(1).replace(\"_\", \" \").lower()\n",
    "                rels[term] = float(item.get(\"weight\", 0.0))\n",
    "            self._cache[w] = rels\n",
    "            self._save_cache()\n",
    "            time.sleep(0.05)\n",
    "            return rels\n",
    "        except Exception:\n",
    "            self._cache[w] = {}\n",
    "            return {}\n",
    "\n",
    "    def semantic_similarity(self, word1: str, word2: str) -> float:\n",
    "        w1 = word1.lower().strip()\n",
    "        w2 = word2.lower().strip()\n",
    "        if not w1 or not w2:\n",
    "            return 0.0\n",
    "        if w1 == w2:\n",
    "            return 1.0\n",
    "\n",
    "        m1 = self._related_map(w1)\n",
    "        m2 = self._related_map(w2)\n",
    "\n",
    "        s = max(m1.get(w2, 0.0), m2.get(w1, 0.0))\n",
    "        return min(1.0, s / 10.0)  # normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polygon solver - composes the three engines into square/pentagon/hexagon attempts.\n",
    "\n",
    "class VerboseLowPolygonalTranslator:\n",
    "    \"\"\"VERBOSE pun translator with scalable phonetics + semantics.\"\"\"\n",
    "\n",
    "    def __init__(self, lexique_path: str):\n",
    "        self.bilingual_dict = RealBilingualDict('en', 'fr')\n",
    "        self.phonetic_dict = LexiquePhoneticIndex(lexique_path)\n",
    "        self.semantic_dict = ConceptNetSemantic(lang=\"fr\")\n",
    "        self.MIN_SEMANTIC_SIM = 0.25  # ConceptNet-normalized scale\n",
    "\n",
    "    def translate_pun_verbose(self, meaning1: str, meaning2: str, max_polygon: int = 8):\n",
    "        print(f\"\\n{'‚ñ¨'*70}\")\n",
    "        print(\"üîç POLYGON TRANSLATION ATTEMPTS\")\n",
    "        print(f\"{'‚ñ¨'*70}\")\n",
    "        print(f\"   Meanings: '{meaning1}' ‚Üî '{meaning2}'\")\n",
    "        print(f\"   Will try polygons 4 through {max_polygon}\")\n",
    "        print(f\"{'‚ñ¨'*70}\\n\")\n",
    "\n",
    "        for level in range(4, min(max_polygon + 1, 9)):\n",
    "            polygon_name = [\"SQUARE\", \"PENTAGON\", \"HEXAGON\", \"HEPTAGON\", \"OCTAGON\"][level-4]\n",
    "\n",
    "            print(f\"\\nüî∏ Attempting {polygon_name} ({level}-gon)...\")\n",
    "            print(f\"   {'‚îÄ'*66}\")\n",
    "\n",
    "            if level == 4:\n",
    "                result = self._attempt_square_verbose(meaning1, meaning2)\n",
    "            elif level == 5:\n",
    "                result = self._attempt_pentagon_verbose(meaning1, meaning2)\n",
    "            elif level == 6:\n",
    "                result = self._attempt_hexagon_verbose(meaning1, meaning2)\n",
    "            elif level == 7:\n",
    "                result = self._attempt_heptagon_verbose(meaning1, meaning2)\n",
    "            elif level == 8:\n",
    "                result = self._attempt_octagon_verbose(meaning1, meaning2)\n",
    "            else:\n",
    "                print(f\"   Skipping {polygon_name} (not implemented)\")\n",
    "                result = None\n",
    "\n",
    "            if result:\n",
    "                print(f\"\\n   ‚úÖ SUCCESS at {polygon_name}!\")\n",
    "                print(f\"   {'‚îÄ'*66}\\n\")\n",
    "                return result, None\n",
    "            else:\n",
    "                print(f\"   ‚ùå {polygon_name} failed - no solution found\")\n",
    "                print(f\"   {'‚îÄ'*66}\")\n",
    "\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"‚ö†Ô∏è  ALL POLYGONS FAILED (4-8)\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(\"   Using fallback: LITERAL TRANSLATION\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "\n",
    "        t1 = self.bilingual_dict.translate(meaning1)[0]\n",
    "        fallback = FallbackTranslation(\n",
    "            strategy=\"Literal Translation\",\n",
    "            translation=f\"{t1}\",\n",
    "            explanation=f\"No pun solution found. Translated '{meaning1}' literally to '{t1}'\"\n",
    "        )\n",
    "        return None, fallback\n",
    "\n",
    "    def _attempt_square_verbose(self, m1: str, m2: str):\n",
    "        print(\"   Method: Direct translation of both meanings\")\n",
    "        t1s = self.bilingual_dict.translate(m1)\n",
    "        t2s = self.bilingual_dict.translate(m2)\n",
    "        print(f\"   '{m1}' ‚Üí {t1s}\")\n",
    "        print(f\"   '{m2}' ‚Üí {t2s}\")\n",
    "\n",
    "        for t1 in t1s:\n",
    "            for t2 in t2s:\n",
    "                if t1 == t2:\n",
    "                    print(f\"   Found: '{t1}' = '{t2}' (same word!)\")\n",
    "                    return TranslationCandidate(\n",
    "                        pun_word=t1, polygon_level=4,\n",
    "                        path=[m1, t1, t2, m2],\n",
    "                        explanation=\"Square: direct translation match\",\n",
    "                        confidence=1.0\n",
    "                    )\n",
    "        print(\"   No direct match found\")\n",
    "        return None\n",
    "\n",
    "    def _attempt_pentagon_verbose(self, m1: str, m2: str):\n",
    "        print(\"   Method: Translate ‚Üí homophone (Lexique) ‚Üí semantic check (ConceptNet)\")\n",
    "        t1s = self.bilingual_dict.translate(m1)\n",
    "        t2s = self.bilingual_dict.translate(m2)\n",
    "        print(f\"   '{m1}' ‚Üí {t1s}\")\n",
    "        print(f\"   '{m2}' ‚Üí {t2s}\")\n",
    "\n",
    "        for t1 in t1s:\n",
    "            homophones = self.phonetic_dict.find_homophones(t1)\n",
    "            print(f\"   Homophones of '{t1}' (Lexique): {homophones[:12]}{'...' if len(homophones) > 12 else ''}\")\n",
    "\n",
    "            for homophone in homophones:\n",
    "                for t2 in t2s:\n",
    "                    sim = self.semantic_dict.semantic_similarity(homophone, t2)\n",
    "                    print(f\"   Similarity('{homophone}', '{t2}'): {sim:.2f}\")\n",
    "                    if sim >= self.MIN_SEMANTIC_SIM:\n",
    "                        print(f\"   ‚úì Semantic match! {sim:.2f} ‚â• {self.MIN_SEMANTIC_SIM}\")\n",
    "                        return TranslationCandidate(\n",
    "                            pun_word=homophone, polygon_level=5,\n",
    "                            path=[m1, t1, homophone, t2, m2],\n",
    "                            explanation=\"Pentagon: translation ‚Üí homophone ‚Üí semantic match\",\n",
    "                            confidence=sim\n",
    "                        )\n",
    "\n",
    "        print(\"   No homophone passed semantic threshold\")\n",
    "        return None\n",
    "\n",
    "    def _attempt_hexagon_verbose(self, m1: str, m2: str):\n",
    "        print(\"   Method: Translate ‚Üí synonym (WordNet EN) ‚Üí translate ‚Üí homophone (Lexique) ‚Üí semantic check (ConceptNet)\")\n",
    "        # Step 1: translate target meaning (B) once\n",
    "        t2s = self.bilingual_dict.translate(m2)\n",
    "        print(f\"   Target meaning '{m2}' ‚Üí {t2s}\")\n",
    "\n",
    "        # Step 2: generate English synonym candidates for m1 (plus m1 itself)\n",
    "        base = m1.lower().strip()\n",
    "        syns = set()\n",
    "        for ss in wn.synsets(base):\n",
    "            for lem in ss.lemmas():\n",
    "                name = lem.name().replace(\"_\", \" \").lower().strip()\n",
    "                # keep simple single-token synonyms to reduce drift\n",
    "                if name.isalpha() and 3 <= len(name) <= 20:\n",
    "                    syns.add(name)\n",
    "        syn_list = [base] + sorted(syns - {base})\n",
    "        syn_list = syn_list[:12]  # keep bounded & fast\n",
    "        print(f\"   Synonym candidates for '{m1}': {syn_list}\")\n",
    "\n",
    "        # Step 3: for each synonym, translate to French, then search homophones and validate semantically\n",
    "        for syn_en in syn_list:\n",
    "            fr_syns = self.bilingual_dict.translate(syn_en)\n",
    "            print(f\"   '{syn_en}' ‚Üí {fr_syns}\")\n",
    "\n",
    "            for fr in fr_syns:\n",
    "                homophones = self.phonetic_dict.find_homophones(fr)\n",
    "                if homophones:\n",
    "                    preview = homophones[:12]\n",
    "                    print(f\"   Homophones of '{fr}' (Lexique): {preview}{'...' if len(homophones) > 12 else ''}\")\n",
    "                else:\n",
    "                    print(f\"   Homophones of '{fr}' (Lexique): []\")\n",
    "                    continue\n",
    "\n",
    "                for homophone in homophones:\n",
    "                    for t2 in t2s:\n",
    "                        sim = self.semantic_dict.semantic_similarity(homophone, t2)\n",
    "                        print(f\"   Similarity('{homophone}', '{t2}'): {sim:.2f}\")\n",
    "                        if sim >= self.MIN_SEMANTIC_SIM:\n",
    "                            print(f\"   ‚úì Semantic match! {sim:.2f} ‚â• {self.MIN_SEMANTIC_SIM}\")\n",
    "                            return TranslationCandidate(\n",
    "                                pun_word=homophone, polygon_level=6,\n",
    "                                path=[m1, syn_en, fr, homophone, t2, m2],\n",
    "                                explanation=\"Hexagon: synonym ‚Üí translation ‚Üí homophone ‚Üí semantic match\",\n",
    "                                confidence=sim\n",
    "                            )\n",
    "\n",
    "        print(\"   No synonym/homophone passed semantic threshold\")\n",
    "        return None\n",
    "\n",
    "    # Copy this code and add it to your notebook after the hexagon method\n",
    "\n",
    "    def _attempt_heptagon_verbose(self, m1: str, m2: str):\n",
    "        \"\"\"\n",
    "        Heptagon (7-gon): 3 semantic/phonetic leaps\n",
    "        Path: m1 ‚Üí t1 ‚Üí FR_synonym ‚Üí homophone ‚Üí FR_synonym ‚Üí t2 ‚Üí m2\n",
    "        \"\"\"\n",
    "        print(\"   Method: Translate ‚Üí FR synonym ‚Üí homophone ‚Üí FR synonym ‚Üí check\")\n",
    "        \n",
    "        t1s = self.bilingual_dict.translate(m1)\n",
    "        t2s = self.bilingual_dict.translate(m2)\n",
    "        print(f\"   '{m1}' ‚Üí {t1s}\")\n",
    "        print(f\"   '{m2}' ‚Üí {t2s}\")\n",
    "        \n",
    "        for t1 in t1s[:3]:\n",
    "            related1 = self.semantic_dict._related_map(t1)\n",
    "            fr_syns1 = [w for w, score in related1.items() if score > 3.0][:10]\n",
    "            \n",
    "            if fr_syns1:\n",
    "                print(f\"   FR synonyms of '{t1}': {fr_syns1[:5]}...\")\n",
    "            \n",
    "            for fr_syn in fr_syns1:\n",
    "                homos = self.phonetic_dict.find_homophones(fr_syn)\n",
    "                \n",
    "                if homos:\n",
    "                    print(f\"   Homophones of '{fr_syn}': {homos[:5]}...\")\n",
    "                \n",
    "                for homo in homos[:10]:\n",
    "                    related_h = self.semantic_dict._related_map(homo)\n",
    "                    fr_syns_h = [w for w, score in related_h.items() if score > 3.0][:10]\n",
    "                    \n",
    "                    for syn_h in fr_syns_h:\n",
    "                        for t2 in t2s:\n",
    "                            sim = self.semantic_dict.semantic_similarity(syn_h, t2)\n",
    "                            \n",
    "                            if sim >= self.MIN_SEMANTIC_SIM:\n",
    "                                print(f\"   ‚úì 7-gon match! {sim:.2f} ‚â• {self.MIN_SEMANTIC_SIM}\")\n",
    "                                return TranslationCandidate(\n",
    "                                    pun_word=homo,\n",
    "                                    polygon_level=7,\n",
    "                                    path=[m1, t1, fr_syn, homo, syn_h, t2, m2],\n",
    "                                    explanation=\"Heptagon: 3 leaps (synonym‚Üíhomophone‚Üísynonym)\",\n",
    "                                    confidence=sim * 0.8\n",
    "                                )\n",
    "        \n",
    "        print(\"   No 7-gon path found\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    def _attempt_octagon_verbose(self, m1: str, m2: str):\n",
    "        \"\"\"\n",
    "        Octagon (8-gon): 4 semantic/phonetic leaps\n",
    "        Path: m1 ‚Üí t1 ‚Üí syn1 ‚Üí homo1 ‚Üí syn2 ‚Üí homo2 ‚Üí t2 ‚Üí m2\n",
    "        \"\"\"\n",
    "        print(\"   Method: Translate ‚Üí syn ‚Üí homo ‚Üí syn ‚Üí homo ‚Üí check\")\n",
    "        print(\"   ‚ö†Ô∏è  8-gon paths are very creative (potentially tenuous)\")\n",
    "        \n",
    "        t1s = self.bilingual_dict.translate(m1)\n",
    "        t2s = self.bilingual_dict.translate(m2)\n",
    "        print(f\"   '{m1}' ‚Üí {t1s}\")\n",
    "        print(f\"   '{m2}' ‚Üí {t2s}\")\n",
    "        \n",
    "        # Very limited search\n",
    "        for t1 in t1s[:2]:\n",
    "            related1 = self.semantic_dict._related_map(t1)\n",
    "            syns1 = [w for w, score in related1.items() if score > 4.0][:5]\n",
    "            \n",
    "            if syns1:\n",
    "                print(f\"   Synonyms of '{t1}': {syns1[:3]}...\")\n",
    "            \n",
    "            for syn1 in syns1:\n",
    "                homos1 = self.phonetic_dict.find_homophones(syn1)[:5]\n",
    "                \n",
    "                for homo1 in homos1:\n",
    "                    related2 = self.semantic_dict._related_map(homo1)\n",
    "                    syns2 = [w for w, score in related2.items() if score > 4.0][:5]\n",
    "                    \n",
    "                    for syn2 in syns2:\n",
    "                        homos2 = self.phonetic_dict.find_homophones(syn2)[:5]\n",
    "                        \n",
    "                        for homo2 in homos2:\n",
    "                            for t2 in t2s:\n",
    "                                sim = self.semantic_dict.semantic_similarity(homo2, t2)\n",
    "                                \n",
    "                                if sim >= self.MIN_SEMANTIC_SIM:\n",
    "                                    print(f\"   ‚úì 8-gon match! {sim:.2f} ‚â• {self.MIN_SEMANTIC_SIM}\")\n",
    "                                    return TranslationCandidate(\n",
    "                                        pun_word=homo2,\n",
    "                                        polygon_level=8,\n",
    "                                        path=[m1, t1, syn1, homo1, syn2, homo2, t2, m2],\n",
    "                                        explanation=\"Octagon: 4 leaps (very creative path)\",\n",
    "                                        confidence=sim * 0.6\n",
    "                                    )\n",
    "        \n",
    "        print(\"   No 8-gon path found\")\n",
    "        return None\n",
    "\n",
    "    def translate_sentence(self, sentence: str, pun_word_original: str, pun_word_french: str) -> str:\n",
    "        words = sentence.split()\n",
    "        french_words = []\n",
    "\n",
    "        for word in words:\n",
    "            clean_word = re.sub(r\"[^\\w']\", \"\", word.lower())\n",
    "            if clean_word == pun_word_original.lower():\n",
    "                french_words.append(pun_word_french)\n",
    "            else:\n",
    "                translations = self.bilingual_dict.translate(clean_word)\n",
    "                french_words.append(translations[0] if translations else word)\n",
    "\n",
    "        return \" \".join(french_words)\n",
    "\n",
    "print(\"‚úÖ Scalable translator loaded (Lexique + ConceptNet + WordNet)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Summary: AutoPunDetector\n",
    "\n",
    "This cell defines the pun-word detector that tokenizes the sentence, filters stopwords, then \n",
    "either flags an out-of-vocabulary ‚Äúmade-up‚Äù token as the pun (and guesses what it‚Äôs blending), \n",
    "or falls back to WordNet to pick a real word with two far-apart senses, boosted if it repeats \n",
    "in the sentence.\n",
    "'''\n",
    "\n",
    "class AutoPunDetector:\n",
    "    \"\"\"Automatically proposes a pun word + two distinct meanings.\n",
    "\n",
    "    Improvements:\n",
    "    1) **Made-up / OOV word handling (portmanteau detector)**:\n",
    "       - If a token has *no* WordNet synsets, treat it as a high-probability pun candidate.\n",
    "       - Try to infer two meanings by:\n",
    "         a) finding a close real-word match (string similarity over WordNet lemmas),\n",
    "         b) finding an in-word substring that is itself a real word (e.g., 'impasta' contains 'pasta').\n",
    "\n",
    "       This fixes cases like: \"A fake noodle is an impasta\" ‚Üí pun word should be \"impasta\".\n",
    "\n",
    "    2) **Surface repetition** remains a strong signal (e.g. \"bank ... bank\").\n",
    "\n",
    "    3) For normal in-vocab words, uses WordNet sense distance to propose two distinct sense labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stop = {\n",
    "            \"the\",\"a\",\"an\",\"and\",\"or\",\"but\",\"if\",\"then\",\"else\",\"to\",\"of\",\"in\",\"on\",\"at\",\"for\",\"with\",\n",
    "            \"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"it\",\"this\",\"that\",\"these\",\"those\",\n",
    "            \"i\",\"you\",\"he\",\"she\",\"we\",\"they\",\"me\",\"him\",\"her\",\"us\",\"them\",\"my\",\"your\",\"his\",\"their\",\n",
    "            \"as\",\"by\",\"from\",\"not\",\"no\",\"so\"\n",
    "        }\n",
    "\n",
    "        # Build a lightweight WordNet lemma vocabulary for fuzzy matching.\n",
    "        # (One-time per runtime; cached on the detector instance.)\n",
    "        lemmas = set()\n",
    "        for name in wn.all_lemma_names():\n",
    "            if not name:\n",
    "                continue\n",
    "            if \"_\" in name:\n",
    "                continue\n",
    "            n = name.lower()\n",
    "            if not n.isalpha():\n",
    "                continue\n",
    "            if len(n) < 3:\n",
    "                continue\n",
    "            lemmas.add(n)\n",
    "        self._wn_vocab = sorted(lemmas)\n",
    "\n",
    "    def _tokenize(self, sentence: str) -> List[str]:\n",
    "        return re.findall(r\"[A-Za-z']+\", sentence.lower())\n",
    "\n",
    "    def _clean_tokens(self, sentence: str) -> List[str]:\n",
    "        toks = self._tokenize(sentence)\n",
    "        return [t for t in toks if t and t not in self.stop]\n",
    "\n",
    "    def _is_oov(self, word: str) -> bool:\n",
    "        return len(wn.synsets(word)) == 0\n",
    "\n",
    "    def _best_close_match(self, word: str) -> Optional[str]:\n",
    "        # difflib is fast enough for our lemma list sizes at n=1.\n",
    "        matches = difflib.get_close_matches(word.lower(), self._wn_vocab, n=1, cutoff=0.78)\n",
    "        return matches[0] if matches else None\n",
    "\n",
    "    def _best_subword(self, word: str) -> Optional[str]:\n",
    "        # Look for the longest substring (>=4 chars) inside the word that is itself a WordNet word.\n",
    "        w = word.lower()\n",
    "        best = None\n",
    "        best_len = 0\n",
    "        for i in range(len(w)):\n",
    "            for j in range(i + 4, len(w) + 1):\n",
    "                sub = w[i:j]\n",
    "                if len(sub) <= best_len:\n",
    "                    continue\n",
    "                if sub in self._wn_vocab or wn.synsets(sub):\n",
    "                    best = sub\n",
    "                    best_len = len(sub)\n",
    "        return best\n",
    "\n",
    "    def _best_synset_pair(self, synsets):\n",
    "        best = None\n",
    "        best_d = -1\n",
    "        for i in range(len(synsets)):\n",
    "            for j in range(i + 1, len(synsets)):\n",
    "                d = synsets[i].shortest_path_distance(synsets[j])\n",
    "                if d is None:\n",
    "                    continue\n",
    "                if d > best_d:\n",
    "                    best_d = d\n",
    "                    best = (synsets[i], synsets[j], d)\n",
    "        return best\n",
    "\n",
    "    def _sense_label(self, synset, surface: str) -> str:\n",
    "        \"\"\"Pick a readable label for a synset that's *not* just the surface word.\"\"\"\n",
    "        surface = surface.lower().replace(\" \", \"_\")\n",
    "        for lem in synset.lemmas():\n",
    "            name = lem.name().lower()\n",
    "            if name != surface:\n",
    "                return name.replace(\"_\", \" \")\n",
    "        definition = synset.definition()\n",
    "        short = \" \".join(definition.split()[:6])\n",
    "        return short\n",
    "\n",
    "    def _detect_oov_pun(self, tokens: List[str], counts: Dict[str, int]) -> Optional[Tuple[str, str, str, str]]:\n",
    "        # Score OOV tokens as likely puns, and try to infer two meanings.\n",
    "        best = None  # (score, word, m1, m2, explanation)\n",
    "\n",
    "        for w, cnt in sorted(counts.items(), key=lambda kv: (-kv[1], kv[0])):\n",
    "            if not self._is_oov(w):\n",
    "                continue\n",
    "\n",
    "            close = self._best_close_match(w)\n",
    "            sub = self._best_subword(w)\n",
    "\n",
    "            # Build two meanings if possible.\n",
    "            meanings = []\n",
    "            if close:\n",
    "                meanings.append(close)\n",
    "            if sub and sub != close:\n",
    "                meanings.append(sub)\n",
    "\n",
    "            if len(meanings) < 2:\n",
    "                # If we only got one, still consider it, but penalize.\n",
    "                pass\n",
    "\n",
    "            # Suspicion score: OOV is a huge signal, then repetition, then evidence of portmanteau structure.\n",
    "            score = 50.0\n",
    "            score += 8.0 * max(0, cnt - 1)  # repetition still matters\n",
    "            score += min(6.0, max(0, len(w) - 5)) * 0.8  # longer weird words are more suspicious\n",
    "            if close:\n",
    "                score += 10.0\n",
    "            if sub:\n",
    "                score += 10.0\n",
    "            if len(meanings) < 2:\n",
    "                score -= 12.0\n",
    "\n",
    "            # Prepare output meanings.\n",
    "            m1 = meanings[0] if meanings else w\n",
    "            m2 = meanings[1] if len(meanings) > 1 else (close or sub or w)\n",
    "\n",
    "            explanation = (\n",
    "                f\"Auto-detected '{w}' as a likely pun word because it has no WordNet senses (OOV).\\n\"\n",
    "                f\"Evidence: close real-word match ‚Üí {close!r}; in-word real-word substring ‚Üí {sub!r}; count {cnt}.\\n\"\n",
    "                f\"Using meanings: A='{m1}', B='{m2}'.\"\n",
    "            )\n",
    "\n",
    "            cand = (score, w, m1, m2, explanation)\n",
    "            if (best is None) or (cand[0] > best[0]):\n",
    "                best = cand\n",
    "\n",
    "        if not best:\n",
    "            return None\n",
    "        _, w, m1, m2, explanation = best\n",
    "        return (w, m1, m2, explanation)\n",
    "\n",
    "    def detect(self, sentence: str) -> Optional[Tuple[str, str, str, str]]:\n",
    "        tokens = self._clean_tokens(sentence)\n",
    "        if not tokens:\n",
    "            return None\n",
    "\n",
    "        counts: Dict[str, int] = {}\n",
    "        for t in tokens:\n",
    "            counts[t] = counts.get(t, 0) + 1\n",
    "\n",
    "        # 1) Prefer made-up / OOV pun candidates (portmanteau-style).\n",
    "        oov = self._detect_oov_pun(tokens, counts)\n",
    "        if oov:\n",
    "            return oov\n",
    "\n",
    "        # 2) Otherwise, fall back to WordNet polysemy + repetition scoring.\n",
    "        best = None  # (score, repeat_count, dist, word, meaning1, meaning2, explanation)\n",
    "\n",
    "        for w, cnt in sorted(counts.items(), key=lambda kv: (-kv[1], kv[0])):\n",
    "            synsets = wn.synsets(w)\n",
    "            if len(synsets) < 2:\n",
    "                continue\n",
    "\n",
    "            pair = self._best_synset_pair(synsets[:10])\n",
    "            if not pair:\n",
    "                continue\n",
    "\n",
    "            s1, s2, dist = pair\n",
    "\n",
    "            m1 = self._sense_label(s1, w)\n",
    "            m2 = self._sense_label(s2, w)\n",
    "            if m1 == m2:\n",
    "                m1 = s1.name().split(\".\")[0].replace(\"_\", \" \")\n",
    "                m2 = s2.name().split(\".\")[0].replace(\"_\", \" \")\n",
    "\n",
    "            repetition_bonus = 15 * max(0, cnt - 1)\n",
    "            polysemy_bonus = min(3.0, 0.25 * len(synsets))\n",
    "            score = dist + repetition_bonus + polysemy_bonus\n",
    "\n",
    "            explanation = (\n",
    "                f\"Auto-detected '{w}' as a possible pun word (score {score:.2f}; \"\n",
    "                f\"distance {dist}; count {cnt}).\\n\"\n",
    "                f\"Sense A: {s1.name()} ‚Äî {s1.definition()}\\n\"\n",
    "                f\"Sense B: {s2.name()} ‚Äî {s2.definition()}\"\n",
    "            )\n",
    "\n",
    "            cand = (score, cnt, dist, w, m1, m2, explanation)\n",
    "            if (best is None) or (cand[:3] > best[:3]):\n",
    "                best = cand\n",
    "\n",
    "        if not best:\n",
    "            return None\n",
    "\n",
    "        _, _, _, w, m1, m2, explanation = best\n",
    "        return (w, m1, m2, explanation)\n",
    "\n",
    "print(\"‚úÖ AutoPunDetector loaded (OOV + WordNet-based)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Summary: translate_pun_complete orchestrator\n",
    "\n",
    "This cell is the end-to-end pipeline: it prints the input, calls the auto pun detector, runs \n",
    "the polygon translator on the two meanings if a pun is found, and otherwise falls back to a \n",
    "literal word-by-word translation.\n",
    "'''\n",
    "\n",
    "def translate_pun_complete(sentence: str, lexique_path: str, show_details: bool = True):\n",
    "    \"\"\"Translate a sentence, attempting an automatically detected pun first.\"\"\"\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üìù ENGLISH INPUT\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"   {sentence}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    detector = AutoPunDetector()\n",
    "    result = detector.detect(sentence)\n",
    "\n",
    "    if not result:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"‚ö†Ô∏è  NO PUN CANDIDATE DETECTED - DOING LITERAL TRANSLATION\")\n",
    "        print(f\"{'='*70}\")\n",
    "        translator = RealBilingualDict('en','fr')\n",
    "        words = sentence.split()\n",
    "        french_words = []\n",
    "        print(\"\\nWord-by-word translation:\")\n",
    "        for word in words:\n",
    "            clean = re.sub(r\"[^\\w']\", \"\", word.lower())\n",
    "            trans = translator.translate(clean)[0]\n",
    "            french_words.append(trans)\n",
    "            print(f\"   {clean} ‚Üí {trans}\")\n",
    "        return \" \".join(french_words)\n",
    "\n",
    "    pun_word, meaning1, meaning2, explain = result\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üéØ PUN DETECTED (AUTO)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"   Pun word: {pun_word}\")\n",
    "    print(f\"   Meaning A: {meaning1}\")\n",
    "    print(f\"   Meaning B: {meaning2}\")\n",
    "    print(f\"\\n   Details:\\n{explain}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    translator = VerboseLowPolygonalTranslator(lexique_path=lexique_path)\n",
    "    candidate, fallback = translator.translate_pun_verbose(meaning1, meaning2)\n",
    "\n",
    "    if candidate:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"‚úÖ FINAL PUN TRANSLATION\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"   French pun word: {candidate.pun_word}\")\n",
    "        print(f\"   Confidence: {candidate.confidence:.2f}\")\n",
    "        print(f\"   Path: {' ‚Üí '.join(candidate.path)}\")\n",
    "        print(f\"   Explanation: {candidate.explanation}\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        french_sentence = translator.translate_sentence(sentence, pun_word, candidate.pun_word)\n",
    "        print(f\"\\nüìå Full sentence: {french_sentence}\")\n",
    "        return french_sentence\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"‚ö†Ô∏è  FALLBACK (NO PUN FOUND)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"   Strategy: {fallback.strategy}\")\n",
    "    print(f\"   Explanation: {fallback.explanation}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    translator2 = RealBilingualDict('en','fr')\n",
    "    words = sentence.split()\n",
    "    return \" \".join(translator2.translate(re.sub(r\"[^\\w']\", \"\", w.lower()))[0] for w in words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Demo\n",
    "\n",
    "1) Download Lexique 3 from lexique.org (free).\n",
    "2) Upload the TSV/CSV into your environment.\n",
    "3) Set `LEXIQUE_PATH` below to the uploaded file path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Summary: example run #1\n",
    "\n",
    "This cell sets the Lexique file path and runs the full pipeline on example #1\n",
    "'''\n",
    "\n",
    "\n",
    "# --- Set this to your local Lexique 3 file path (TSV/CSV) ---\n",
    "LEXIQUE_PATH = \"/content/Lexique383.tsv\"  # <-- update this path\n",
    "\n",
    "translate_pun_complete(\"Time flies like an arrow.\", lexique_path=LEXIQUE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Summary: example run #2\n",
    "\n",
    "This cell sets the Lexique file path and runs the full pipeline on example #2\n",
    "'''\n",
    "\n",
    "translate_pun_complete(\"I went to the bank to watch the river bank.\", lexique_path=LEXIQUE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Interactive Form\n",
    "\n",
    "Set `LEXIQUE_PATH` once, then try different sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Summary: Colab-style UI form\n",
    "\n",
    "This cell creates a simple form input so you can paste any sentence and run \n",
    "the same pipeline interactively with optional detailed logs.\n",
    "\n",
    "If you want, I can also give you one sentence that explains the ‚Äúpolygon‚Äù \n",
    "idea in plain English (because that‚Äôs usually what people ask about first).\n",
    "'''\n",
    "\n",
    "\n",
    "# @title üé≠ Enter Your Pun (Scalable) { display-mode: \"form\" }\n",
    "\n",
    "LEXIQUE_PATH = \"/content/Lexique383.tsv\"  # @param {type:\"string\"}\n",
    "pun_sentence = \"I went to the bank to watch the river bank.\" # @param {type:\"string\"}\n",
    "show_detailed_output = True # @param {type:\"boolean\"}\n",
    "\n",
    "result = translate_pun_complete(pun_sentence, lexique_path=LEXIQUE_PATH, show_details=show_detailed_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
